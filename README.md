# ğŸ§  SARSA-Based Reinforcement Learning Algorithms

This repository provides clear implementations of various **SARSA-based reinforcement learning algorithms** using Python and OpenAI Gym. It includes:

- SARSA(0)
- SARSA-Max (also known as Q-learning)
- Expected SARSA

All algorithms use an Îµ-greedy exploration policy and are designed for **discrete action-space environments** like `Taxi-v3`, `CliffWalking-v0`, and `FrozenLake-v1`.

---

## ğŸ“Œ Algorithms Explained

### 1. ğŸ” SARSA(0)
SARSA is an **on-policy TD control algorithm** that updates Q-values using the next action selected by the same policy.

**Update Rule:**

Q(s, a) â† Q(s, a) + Î± * [r + Î³ * Q(sâ€², aâ€²) âˆ’ Q(s, a)]

Where:
- `s, a` = current state and action  
- `sâ€², aâ€²` = next state and action  
- `r` = reward  
- `Î±` = learning rate  
- `Î³` = discount factor  

---

### 2. ğŸ“ˆ SARSA-Max (Q-Learning)
Q-Learning is an **off-policy** variant of SARSA. It uses the maximum future Q-value to update the current state-action pair.

**Update Rule:**

Q(s, a) â† Q(s, a) + Î± * [r + Î³ * max_aâ€² Q(sâ€², aâ€²) âˆ’ Q(s, a)]

It assumes the agent will always act optimally in the future (greedy update).

---

### 3. ğŸ“Š Expected SARSA
Expected SARSA averages over all possible actions using their probabilities under the current Îµ-greedy policy. This leads to more **stable learning**.

**Update Rule:**

Q(s, a) â† Q(s, a) + Î± * [r + Î³ * âˆ‘_aâ€² Ï€(aâ€²|sâ€²) * Q(sâ€², aâ€²) âˆ’ Q(s, a)]

Where `Ï€(aâ€²|sâ€²)` is the probability of action `aâ€²` under the current Îµ-greedy policy.

---

## ğŸ§ª Example Environments

These algorithms have been tested on:

- `Taxi-v3`
- `CliffWalking-v0`
- `FrozenLake-v1` (both deterministic and stochastic)

All environments are part of [OpenAI Gym](https://www.gymlibrary.dev/).

---


pip install -r requirements.txt
